{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "manglish_lyrics_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNB2NoY84NOx"
      },
      "source": [
        "##### Download and UnZip the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NweAqqHYYgm"
      },
      "source": [
        "!wget https://github.com/nandakishormpai2001/manglish_lyrics_generator/raw/main/model/train_notebook/data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nst343m9Zgdo"
      },
      "source": [
        "!unzip \"data.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_-XSssAF25r"
      },
      "source": [
        "##### Pip Install the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3eunFTZZX8A"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQd50gEfF-CR"
      },
      "source": [
        "##### Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iayTSy1fwso_"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka3EGkwmVcLu"
      },
      "source": [
        "###### Define the Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHItmqLzpPpZ"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    # Set hidden layer parameters\n",
        "    def __init__(self, dataset):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 2\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        \n",
        "        # Embedding layer converts word indexes to word vectors.\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        # Define the LSTM, the number of expected features and that of layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_size,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        # Define the Fully Connected layer\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    # Defines the network structure, here consisting of the embedding layer, LSTM layer and the fully connected layer. \n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjM9pIRIVjfW"
      },
      "source": [
        "##### Define the Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJzEzoZRwrjc"
      },
      "source": [
        "class Dataset():\n",
        "    def __init__(self):\n",
        "        self.words = self.load_words()\n",
        "        text_file = open(\"data/words.txt\", \"w\")\n",
        "        text_file.write(\" \".join(self.words))\n",
        "        text_file.close()\n",
        "        self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        # index_to_word and word_to_index converts words to number indexes and visa versa.\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "        with open('data/index_to_word.json', 'wb') as iw:\n",
        "            pickle.dump(self.index_to_word, iw)\n",
        "        with open('data/word_to_index.json', 'wb') as wi:\n",
        "            pickle.dump(self.word_to_index, wi)    \n",
        "        \n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "        self.sequence_length=100\n",
        "\n",
        "    \n",
        "    def load_words(self):\n",
        "\n",
        "        \"\"\" Returns words [list] - list of words\n",
        "        Open the files in the dataset, and for each file read its contents, \n",
        "        replace the endline with an ' <EOL> ' and then split each \n",
        "        words by space. These words are then returned in a list.\"\"\"\n",
        "        \n",
        "        files = os.listdir(\"data/lyrics\")\n",
        "        words = []\n",
        "        for file in files:\n",
        "            with open(\"data/lyrics/\"+file,'r') as txtfile:\n",
        "                lyrics = txtfile.read()\n",
        "                lyrics = lyrics.replace(\"\\n\",\" <EOL> \")\n",
        "                words = words + lyrics.split(\" \")\n",
        "        return words\n",
        "    \n",
        "    # Gets a list of unique words\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.sequence_length\n",
        "\n",
        "    # To support the indexing such that dataset[i] can be used to get ith sample.\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n",
        "        )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVjn5254lAZz"
      },
      "source": [
        "##### Define the function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYNdnyfZwsDY"
      },
      "source": [
        "def train(dataset, model):\n",
        "    model.train()\n",
        "    dataloader = DataLoader(dataset, batch_size=128,shuffle=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    num_of_epochs = 2\n",
        "\n",
        "    # Declare lists to store the respective values, inorder to plot graphs later\n",
        "    epochs = []\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Iterate the loop, for the number of epochs\n",
        "    for epoch in range(num_of_epochs):\n",
        "        state_h, state_c = model.init_state(100)\n",
        "        state_h_val, state_c_val = model.init_state(100)\n",
        "        cnt = 0\n",
        "        val_cnt=0\n",
        "        tot_loss = 0\n",
        "        tot_val_loss = 0\n",
        "        \n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "            # Training (85% of the dataset)\n",
        "            if (batch<46):\n",
        "                # Sets the gradients of all optimized tensors to zero\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "                # Compute loss (here CrossEntropyLoss)\n",
        "                loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "                state_h = state_h.detach()\n",
        "                state_c = state_c.detach()\n",
        "\n",
        "                # Compute gradient of the loss with respect to all the learnable parameters of the model. \n",
        "                loss.backward()\n",
        "                # update its parameters\n",
        "                optimizer.step()\n",
        "\n",
        "                # Calcuate the total loss\n",
        "                tot_loss = tot_loss + loss\n",
        "                cnt = cnt + 1\n",
        "            \n",
        "            # Validation using 15% of the dataset\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    y_pred, (state_h_val, state_c_val) = model(x, (state_h_val, state_c_val))\n",
        "                    # Compute loss (here CrossEntropyLoss)\n",
        "                    val_loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "                state_h_val = state_h_val.detach()\n",
        "                state_c_val = state_c_val.detach()\n",
        "\n",
        "                # Calcuate the total loss\n",
        "                tot_val_loss = tot_val_loss + val_loss\n",
        "                val_cnt = val_cnt + 1\n",
        "        \n",
        "        # Append the epochs number and the loss in thier lists\n",
        "        epochs.append(epoch)\n",
        "        losses.append((tot_loss/cnt))\n",
        "        val_losses.append((tot_val_loss/val_cnt))\n",
        "        print(\"epoch = {}  loss = {} val_loss = {} \".format(epoch, (tot_loss/cnt),(tot_val_loss/val_cnt)))\n",
        "\n",
        "    # Plot a Validation Loss vs Epochs graph \n",
        "    plt.plot(epochs, val_losses, color='green', linewidth = 3, \n",
        "         marker='o', markerfacecolor='blue', markersize=8) \n",
        "    plt.xlabel('epochs ---->',color='m',fontsize='xx-large' ) \n",
        "    plt.ylabel('validation loss ------>',color='m',fontsize='xx-large') \n",
        "    axes = plt.gca()        # 'gca' - get current axes\n",
        "    axes.set_facecolor('c') #'c' - cyan\n",
        "    axes.tick_params(axis='y', which='both', colors='tomato')\n",
        "    axes.tick_params(axis='x', which='both', colors='#20ff14')\n",
        "    plt.title(\"Val Loss vs Epoch\",color='m',fontsize='xx-large')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9ixfIDsl5Wm"
      },
      "source": [
        "##### Define the function to generate the lyrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRjALcEHIY_2"
      },
      "source": [
        "# to avoid gradients update\n",
        "@torch.no_grad()\n",
        "def generate(model,keywords,next_words=100):\n",
        "    # loading lyrics corpus\n",
        "    words_file = open(\"data/words.txt\",\"r\")\n",
        "    words = words_file.read().split(\" \")\n",
        "\n",
        "    # returning failed message as keyword not in dataset\n",
        "    for keyword in keywords.split(\" \"):\n",
        "        if (keyword not in words):\n",
        "            while True:\n",
        "                n = random.randint(0,len(words))\n",
        "                message = '\"'+keyword+'\" Keyword not found in dataset. Try words like '+\", \".join(words[n:n+3])\n",
        "                if(\"<EOL>\" not in message and \"(\" not in message and \")\" not in message):\n",
        "                    return message\n",
        "\n",
        "    loaded_model = model\n",
        "    loaded_model.load_state_dict(torch.load(\"MODEL-NAME-HERE.pth\"))\n",
        "    loaded_model.eval()\n",
        "    words = keywords.split(\" \")\n",
        "\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    # Loading dictionaries for word to index and vice versa \n",
        "    with open('data/word_to_index.json', 'rb') as wi:\n",
        "        word_to_index = pickle.load(wi)\n",
        "    with open('data/index_to_word.json', 'rb') as iw:\n",
        "        index_to_word = pickle.load(iw)\n",
        "    # for loop to generate lyrics\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[word_to_index[w] for w in words[i:]]])\n",
        "        y_pred, (state_h, state_c) = loaded_model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(index_to_word[word_index])\n",
        "    \n",
        "    # print(len(words),words[50],words[100])\n",
        "    lyrics = \" \".join(words)\n",
        "    # formatting the lyrics\n",
        "    lyrics = lyrics.replace(\" <EOL> \",\"\\n\")\n",
        "    lyrics = lyrics.replace(\"<EOL> \",\"\\n\")\n",
        "    lyrics = lyrics.replace(\" <EOL>\",\"\\n\")\n",
        "\n",
        "    words_file.close()\n",
        "    # returning as a string\n",
        "    return lyrics"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da4_5Y0nGZCx"
      },
      "source": [
        "dataset = Dataset()\n",
        "model = Model(dataset)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "389-6-kE2Mix"
      },
      "source": [
        "train(dataset, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wt2ByACZ1g1"
      },
      "source": [
        "#Save the trained model\n",
        "torch.save(model.state_dict(), \"MODEL-NAME-HERE.pth\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7SV9loBEbFy"
      },
      "source": [
        "loaded_model = Model(dataset)\n",
        "loaded_model.load_state_dict(torch.load(\"MODEL-NAME-HERE.pth\"))\n",
        "lyrics_generated = generate(model,\"oru Ishtam dhim\") # A random string with words chosen from the words.txt, separated by spaces.\n",
        "#print(\"words=\",len(lyrics_generated.split(\" \")))\n",
        "print(lyrics_generated)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}